# space-restore

# Восстановление пробелов в тексте

## Описание проекта
В этом проекте решается задача восстановления пробелов в русском тексте.  
На вход подаётся строка без пробелов, модель должна вставить их в правильных местах.  

Подход:
- собраны данные из разных источников (**Авито ML Cup**, **Лента.ру**, **русская Википедия**);
- построены пары «слитный текст → нормализованный текст»;
- обучена **BiLSTM-модель** на символах в два этапа:
  - стадия A — обучение на «чистом» корпусе (Лента + Википедия);
  - стадия B — дообучение на смеси (Авито + часть чистого корпуса);
- дополнительно применяется постпроцессинг (морфология, бренды, пунктуация), чтобы улучшить читаемость результата.

Итогом является готовая модель и сабмишен для датасета из задания.

---

## Данные
В папке **Data** находятся:
- `brands.txt` — список брендов, используемый для постпроцессинга;
- `dataset_1937770_3.txt` — исходный датасет из задания;
- `submission.csv` — итоговый сабмишен.

### Источники данных
- [Avito ML Cup](https://www.kaggle.com/competitions/avito-duplicate-ads-detection) — тексты объявлений;
- [Корпус новостей Лента.ру](https://www.kaggle.com/datasets/yutkin/corpus-of-russian-news-articles-from-lenta) — новостные статьи;
- [Wikipedia Monthly Dumps](https://huggingface.co/datasets/omarkamali/wikipedia-monthly) — свежая версия русской Википедии.

Все данные использовались только в исследовательских целях. Лицензии соответствуют источникам:
- Wikipedia — [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/),
- Лента.ру — открытый корпус,
- Avito ML Cup — **MIT**.

---

## Почему BiLSTM, а не Transformer
Я рассматривала метод **ByT5-small (Transformer)**, однако он оказался слишком ресурсоёмким: обучение и инференс занимают слишком много времени на сервере.  
По моим критериям этот вариант неэффективен, хотя, вероятно, он мог бы показать более высокий F1.  

Поэтому я выбрала **BiLSTM**, который быстрее и проще в применении для данной задачи.

---

## Литература
- Применение BiLSTM для сегментации текста:  
  *Kawakami, K., Dyer, C., & Blunsom, P. (2019). Learning to Segment Input Sequences into Meaningful Units.*  
  [arXiv:1902.08823](https://arxiv.org/abs/1902.08823)

- ByT5 (включая ByT5-small):  
  *Xue, L., Constant, N., Roberts, A., et al. (2021). ByT5: Towards a token-free future with pre-trained byte-to-byte models.*  
  [arXiv:2105.13626](https://arxiv.org/abs/2105.13626)
